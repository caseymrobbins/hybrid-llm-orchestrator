-e \n# === src/clients/anthropic.py ===\n
# src/clients/anthropic.py

import asyncio
import logging
from typing import Optional, Dict, Any
from anthropic import AsyncAnthropic, AnthropicError, RateLimitError
from .base import LlmClient

logger = logging.getLogger(__name__)

class AnthropicClient(LlmClient):
    """Client for Anthropic's Claude models with comprehensive error handling and retry logic."""

    def __init__(self, api_key: str, model_name: Optional[str] = None):
        if not api_key:
            raise ValueError("API key is required for AnthropicClient.")
        
        self.client = AsyncAnthropic(api_key=api_key)
        self.model_name = model_name or "claude-3-sonnet-20240229"
        self.max_retries = 3
        self.base_delay = 1

    async def generate(
        self, 
        prompt: str, 
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        Generate text using Anthropic's Messages API.
        
        Args:
            prompt: The user prompt
            system_prompt: Optional system message
            **kwargs: Additional parameters (model, max_tokens, temperature, etc.)
            
        Returns:
            Generated text response
            
        Raises:
            ConnectionError: When API communication fails
            ValueError: When response is invalid
        """
        model = kwargs.get("model", self.model_name)
        max_tokens = kwargs.get("max_tokens", 1024)
        temperature = kwargs.get("temperature", 0.7)
        top_p = kwargs.get("top_p", 0.9)

        # Build messages array
        messages = []
        messages.append({"role": "user", "content": prompt})

        # Prepare API call parameters
        api_params = {
            "model": model,
            "max_tokens": max_tokens,
            "messages": messages
        }

        # Add system prompt if provided
        if system_prompt:
            api_params["system"] = system_prompt

        # Add optional parameters
        if "temperature" in kwargs:
            api_params["temperature"] = temperature
        if "top_p" in kwargs:
            api_params["top_p"] = top_p

        # Retry with exponential backoff
        for attempt in range(self.max_retries + 1):
            try:
                response = await self.client.messages.create(**api_params)
                
                # Validate response structure
                if not response.content:
                    raise ValueError("No response content received from Anthropic API")
                
                if not isinstance(response.content, list) or len(response.content) == 0:
                    raise ValueError("Invalid response content structure from Anthropic API")
                
                # Extract text from the first content block
                content_block = response.content[0]
                if not hasattr(content_block, 'text'):
                    raise ValueError("Response content block does not contain text")
                
                return content_block.text.strip()
                
            except RateLimitError as e:
                if attempt < self.max_retries:
                    delay = self.base_delay * (2 ** attempt)
                    logger.warning(f"Anthropic rate limit exceeded. Retrying in {delay} seconds... (attempt {attempt + 1}/{self.max_retries})")
                    await asyncio.sleep(delay)
                    continue
                else:
                    raise ConnectionError(f"Anthropic rate limit exceeded after {self.max_retries} retries: {e}")
                    
            except AnthropicError as e:
                error_msg = f"Anthropic API error: {e}"
                logger.error(error_msg)
                raise ConnectionError(error_msg)
                
            except Exception as e:
                error_msg = f"Unexpected error during Anthropic generation: {e}"
                logger.error(error_msg)
                raise ConnectionError(error_msg)
        
        raise ConnectionError("Maximum retries exceeded")

    async def validate_connection(self) -> bool:
        """
        Validate that the client can connect to Anthropic API.
        
        Returns:
            True if connection is successful, False otherwise
        """
        try:
            # Try a minimal request to validate connection
            await self.generate("Hello", max_tokens=1)
            return True
        except Exception as e:
            logger.error(f"Anthropic connection validation failed: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model configuration.
        
        Returns:
            Dictionary containing model information
        """
        return {
            "model_name": self.model_name,
            "provider": "anthropic",
            "max_retries": self.max_retries
        }-e \n# === src/clients/base.py ===\n
# src/clients/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional

class LlmClient(ABC):
    """
    Abstract base class for all LLM clients.
    
    Defines the common interface that all LLM clients must implement,
    ensuring consistency across different providers and model types.
    """

    @abstractmethod
    async def generate(
        self, 
        prompt: str, 
        system_prompt: Optional[str] = None,
        **kwargs: Any
    ) -> str:
        """
        Generate a response from the LLM based on the provided prompt.
        
        Args:
            prompt: The input text/prompt for the model
            system_prompt: Optional system message or context
            **kwargs: Additional provider-specific parameters such as:
                - model: Override the default model
                - max_tokens: Maximum tokens to generate
                - temperature: Sampling temperature (0.0 to 2.0)
                - top_p: Top-p sampling parameter
                - frequency_penalty: Frequency penalty
                - presence_penalty: Presence penalty
                - timeout: Request timeout in seconds
                
        Returns:
            The text response from the LLM
            
        Raises:
            ConnectionError: When API communication fails
            ValueError: When input parameters are invalid
            TimeoutError: When request times out
        """
        pass

    async def validate_connection(self) -> bool:
        """
        Validate that the client can successfully connect to the LLM service.
        
        Returns:
            True if connection is valid and working, False otherwise
        """
        try:
            # Default implementation - try a simple generation
            await self.generate("Hello", max_tokens=1)
            return True
        except Exception:
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model configuration.
        
        Returns:
            Dictionary containing model information such as:
            - model_name: Name of the current model
            - provider: Name of the provider
            - max_retries: Maximum retry attempts
            - base_url: API base URL (if applicable)
        """
        return {
            "model_name": getattr(self, 'model_name', 'unknown'),
            "provider": self.__class__.__name__.replace('Client', '').lower()
        }

    def get_generation_params(self) -> Dict[str, Any]:
        """
        Get the default generation parameters for this client.
        
        Returns:
            Dictionary of default parameters used for text generation
        """
        return {
            "temperature": getattr(self, 'temperature', 0.7),
            "max_tokens": getattr(self, 'max_tokens', 1024),
            "top_p": getattr(self, 'top_p', 0.9)
        }

    async def health_check(self) -> Dict[str, Any]:
        """
        Perform a comprehensive health check of the client.
        
        Returns:
            Dictionary containing health check results
        """
        try:
            connection_valid = await self.validate_connection()
            model_info = self.get_model_info()
            
            return {
                "status": "healthy" if connection_valid else "unhealthy",
                "connection_valid": connection_valid,
                "model_info": model_info,
                "client_type": self.__class__.__name__
            }
        except Exception as e:
            return {
                "status": "error",
                "connection_valid": False,
                "error": str(e),
                "client_type": self.__class__.__name__
            }

    def __repr__(self) -> str:
        """String representation of the client."""
        model_name = getattr(self, 'model_name', 'unknown')
        return f"{self.__class__.__name__}(model='{model_name}')"-e \n# === src/clients/local.py ===\n
# src/clients/local.py

import asyncio
import logging
import torch
from typing import Dict, Any, Optional, Union
from concurrent.futures import ThreadPoolExecutor
from transformers import (
    pipeline, 
    AutoTokenizer, 
    AutoModelForCausalLM, 
    Pipeline,
    PreTrainedTokenizer,
    PreTrainedModel
)

from .base import LlmClient

logger = logging.getLogger(__name__)

class LocalClientError(Exception):
    """Custom exception for local client errors."""
    pass

class LocalClient(LlmClient):
    """
    Client for running local models via Hugging Face transformers.
    
    Features:
    - Automatic device detection (CUDA, MPS, CPU)
    - Proper async handling with thread pool execution
    - Memory-efficient loading with optional quantization
    - Resource cleanup and management
    - Comprehensive error handling
    """

    def __init__(
        self, 
        model_path: str, 
        use_pipeline: bool = True,
        device: Optional[str] = None,
        torch_dtype: Optional[torch.dtype] = None,
        max_workers: int = 1
    ):
        """
        Initialize the local client.
        
        Args:
            model_path: Path to the model (local path or HuggingFace model ID)
            use_pipeline: Whether to use HuggingFace pipeline (recommended)
            device: Device to use ('cuda', 'mps', 'cpu', or None for auto-detection)
            torch_dtype: Torch data type (None for automatic)
            max_workers: Number of worker threads for async execution
        """
        self.model_path = model_path
        self.use_pipeline = use_pipeline
        self._device = device or self._get_optimal_device()
        self._torch_dtype = torch_dtype or self._get_optimal_dtype()
        self._executor = ThreadPoolExecutor(max_workers=max_workers)
        
        # Model components
        self.pipe: Optional[Pipeline] = None
        self.model: Optional[PreTrainedModel] = None
        self.tokenizer: Optional[PreTrainedTokenizer] = None
        
        # Configuration
        self.model_name = model_path
        self.max_tokens = 512
        self.temperature = 0.7
        self.top_p = 0.95
        
        logger.info(f"Initializing LocalClient for '{model_path}' on device '{self._device}'...")
        
        # Initialize the model
        self._initialize_model()

    def _get_optimal_device(self) -> str:
        """Determine the best available device for inference."""
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"CUDA available: {device_count} devices, {memory:.1f}GB memory")
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            logger.info("MPS (Apple Silicon) available")
            return "mps"
        else:
            logger.info("Using CPU for inference")
            return "cpu"

    def _get_optimal_dtype(self) -> torch.dtype:
        """Determine optimal torch dtype based on device capabilities."""
        if self._device == "cuda":
            # Check if bfloat16 is supported (Ampere or newer)
            if torch.cuda.is_available():
                capability = torch.cuda.get_device_capability()
                if capability[0] >= 8:  # Ampere or newer
                    logger.info("Using bfloat16 for optimal performance")
                    return torch.bfloat16
                else:
                    logger.info("Using float16 for memory efficiency")
                    return torch.float16
        elif self._device == "mps":
            # MPS works well with float16
            return torch.float16
        
        # CPU fallback
        return torch.float32

    def _initialize_model(self) -> None:
        """Initialize the model and tokenizer."""
        try:
            if self.use_pipeline:
                self._initialize_pipeline()
            else:
                self._initialize_manual()
                
            logger.info(f"Local model '{self.model_path}' loaded successfully on {self._device}")
            
        except Exception as e:
            error_msg = f"Failed to load local model '{self.model_path}': {e}"
            logger.error(error_msg)
            raise LocalClientError(error_msg)

    def _initialize_pipeline(self) -> None:
        """Initialize using HuggingFace pipeline (recommended approach)."""
        self.pipe = pipeline(
            "text-generation",
            model=self.model_path,
            device=self._device if self._device != "mps" else -1,  # MPS uses device=-1
            torch_dtype=self._torch_dtype,
            model_kwargs={
                "low_cpu_mem_usage": True,
                "torch_dtype": self._torch_dtype
            }
        )

    def _initialize_manual(self) -> None:
        """Initialize model and tokenizer manually (for more control)."""
        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        
        # Set padding token if not present
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Load model
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=self._torch_dtype,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            device_map="auto" if self._device == "cuda" else None
        )
        
        # Move model to device if not using device_map
        if self._device != "cuda":
            self.model = self.model.to(self._device)

    async def generate(
        self, 
        prompt: str, 
        system_prompt: Optional[str] = None,
        **kwargs: Any
    ) -> str:
        """
        Generate text using the local model.
        
        Args:
            prompt: The input prompt
            system_prompt: Optional system message (will be prepended)
            **kwargs: Additional generation parameters
            
        Returns:
            Generated text response
            
        Raises:
            LocalClientError: When generation fails
            ValueError: When input is invalid
        """
        if not prompt or not prompt.strip():
            raise ValueError("Prompt cannot be empty")

        # Prepare the full prompt
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"

        # Extract generation parameters
        max_new_tokens = kwargs.get("max_new_tokens", kwargs.get("max_tokens", self.max_tokens))
        temperature = kwargs.get("temperature", self.temperature)
        top_p = kwargs.get("top_p", self.top_p)
        do_sample = kwargs.get("do_sample", temperature > 0)

        try:
            # Run generation in thread pool to avoid blocking the async loop
            if self.use_pipeline and self.pipe:
                result = await self._generate_with_pipeline(
                    full_prompt, max_new_tokens, temperature, top_p, do_sample
                )
            elif self.model and self.tokenizer:
                result = await self._generate_manual(
                    full_prompt, max_new_tokens, temperature, top_p, do_sample
                )
            else:
                raise LocalClientError("Model not properly initialized")

            return result.strip()

        except Exception as e:
            error_msg = f"Text generation failed: {e}"
            logger.error(error_msg)
            raise LocalClientError(error_msg)

    async def _generate_with_pipeline(
        self, 
        prompt: str, 
        max_new_tokens: int, 
        temperature: float, 
        top_p: float,
        do_sample: bool
    ) -> str:
        """Generate text using the pipeline."""
        def _sync_generate():
            outputs = self.pipe(
                prompt,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=do_sample,
                return_full_text=False,  # Only return generated text
                clean_up_tokenization_spaces=True
            )
            
            if outputs and isinstance(outputs, list) and len(outputs) > 0:
                if 'generated_text' in outputs[0]:
                    return outputs[0]['generated_text']
                else:
                    raise LocalClientError("Pipeline output missing 'generated_text' field")
            else:
                raise LocalClientError("Pipeline returned empty or invalid output")

        return await asyncio.get_event_loop().run_in_executor(
            self._executor, _sync_generate
        )

    async def _generate_manual(
        self, 
        prompt: str, 
        max_new_tokens: int, 
        temperature: float, 
        top_p: float,
        do_sample: bool
    ) -> str:
        """Generate text using model and tokenizer manually."""
        def _sync_generate():
            # Tokenize input
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True,
                max_length=2048
            ).to(self.model.device)
            
            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature if do_sample else 1.0,
                    top_p=top_p if do_sample else 1.0,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode only the generated tokens
            input_length = inputs['input_ids'].shape[1]
            generated_tokens = outputs[0][input_length:]
            
            return self.tokenizer.decode(
                generated_tokens, 
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )

        return await asyncio.get_event_loop().run_in_executor(
            self._executor, _sync_generate
        )

    async def validate_connection(self) -> bool:
        """
        Validate that the local model is loaded and working.
        
        Returns:
            True if model is working, False otherwise
        """
        try:
            # Try a very simple generation
            result = await self.generate("Hello", max_tokens=5)
            return len(result.strip()) > 0
        except Exception as e:
            logger.error(f"Local model validation failed: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the local model."""
        try:
            # Get memory usage if CUDA
            memory_info = {}
            if self._device == "cuda" and torch.cuda.is_available():
                memory_info = {
                    "allocated_memory_gb": torch.cuda.memory_allocated() / 1e9,
                    "cached_memory_gb": torch.cuda.memory_reserved() / 1e9,
                    "max_memory_gb": torch.cuda.max_memory_allocated() / 1e9
                }

            # Get model parameters count
            param_count = 0
            if self.pipe and hasattr(self.pipe.model, 'num_parameters'):
                param_count = self.pipe.model.num_parameters()
            elif self.model and hasattr(self.model, 'num_parameters'):
                param_count = self.model.num_parameters()

            return {
                "model_name": self.model_name,
                "model_path": self.model_path,
                "provider": "local_huggingface",
                "device": self._device,
                "torch_dtype": str(self._torch_dtype),
                "use_pipeline": self.use_pipeline,
                "parameters": param_count,
                "memory_info": memory_info
            }
            
        except Exception as e:
            logger.warning(f"Could not get complete model info: {e}")
            return {
                "model_name": self.model_name,
                "provider": "local_huggingface",
                "device": self._device,
                "error": str(e)
            }

    def cleanup(self) -> None:
        """Clean up model resources."""
        try:
            logger.info("Cleaning up local model resources...")
            
            # Clear CUDA cache if using GPU
            if self._device == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            # Shutdown thread pool
            if self._executor:
                self._executor.shutdown(wait=True)
            
            # Clear model references
            self.pipe = None
            self.model = None
            self.tokenizer = None
            
            logger.info("Local model resources cleaned up")
            
        except Exception as e:
            logger.warning(f"Error during local model cleanup: {e}")

    def __del__(self):
        """Destructor to ensure cleanup."""
        try:
            self.cleanup()
        except Exception:
            pass  # Ignore errors in destructor-e \n# === src/clients/openai_compatible.py ===\n
# src/clients/openai_compatible.py

import asyncio
import logging
from typing import Optional, Dict, Any
from openai import AsyncOpenAI, OpenAIError, RateLimitError
from .base import LlmClient

logger = logging.getLogger(__name__)

class OpenAICompatibleClient(LlmClient):
    """Client for OpenAI and any OpenAI-compatible APIs (e.g., Grok, Anthropic via proxy)."""

    def __init__(self, api_key: str, base_url: Optional[str] = None):
        if not api_key:
            raise ValueError("API key is required for OpenAICompatibleClient.")
        
        self.client = AsyncOpenAI(
            api_key=api_key, 
            base_url=base_url or "https://api.openai.com/v1"
        )
        self.model_name = self._determine_model_name(base_url or "")
        self.max_retries = 3
        self.base_delay = 1  # Base delay for exponential backoff

    def _determine_model_name(self, base_url: str) -> str:
        """Determine appropriate model based on the provider URL."""
        base_url = base_url.lower()
        
        # Provider-specific model mapping
        if "api.x.ai" in base_url:
            return "grok-beta"  # Correct Grok model name
        elif "api.groq.com" in base_url:
            return "mixtral-8x7b-32768"
        elif "api.together.xyz" in base_url:
            return "mistralai/Mixtral-8x7B-Instruct-v0.1"
        elif "api.anthropic.com" in base_url:
            return "claude-3-sonnet-20240229"
        
        # Default OpenAI model
        return "gpt-4o-mini"

    async def generate(
        self, 
        prompt: str, 
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        Generates text using an OpenAI-compatible chat completion endpoint.
        
        Args:
            prompt: The user prompt
            system_prompt: Optional system message
            **kwargs: Additional parameters (model, max_tokens, temperature, etc.)
            
        Returns:
            Generated text response
            
        Raises:
            ConnectionError: When API communication fails
            ValueError: When response is invalid
        """
        model = kwargs.get("model", self.model_name)
        max_tokens = kwargs.get("max_tokens", 1024)
        temperature = kwargs.get("temperature", 0.7)

        # Build messages array
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        # Retry with exponential backoff
        for attempt in range(self.max_retries + 1):
            try:
                response = await self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    **{k: v for k, v in kwargs.items() 
                       if k not in ['model', 'max_tokens', 'temperature']}
                )
                
                # Validate response structure
                if not response.choices or len(response.choices) == 0:
                    raise ValueError("No response choices received from API")
                
                content = response.choices[0].message.content
                if content is None:
                    raise ValueError("Response content is None")
                
                return content.strip()
                
            except RateLimitError as e:
                if attempt < self.max_retries:
                    delay = self.base_delay * (2 ** attempt)  # Exponential backoff
                    logger.warning(f"Rate limit exceeded. Retrying in {delay} seconds... (attempt {attempt + 1}/{self.max_retries})")
                    await asyncio.sleep(delay)
                    continue
                else:
                    raise ConnectionError(f"Rate limit exceeded after {self.max_retries} retries: {e}")
                    
            except OpenAIError as e:
                # Handle other OpenAI errors
                error_msg = f"OpenAI API error: {e}"
                logger.error(error_msg)
                raise ConnectionError(error_msg)
                
            except Exception as e:
                # Handle unexpected errors
                error_msg = f"Unexpected error during generation: {e}"
                logger.error(error_msg)
                raise ConnectionError(error_msg)
        
        # This should never be reached due to the loop structure, but just in case
        raise ConnectionError("Maximum retries exceeded")

    async def validate_connection(self) -> bool:
        """
        Validate that the client can connect to the API.
        
        Returns:
            True if connection is successful, False otherwise
        """
        try:
            # Try a minimal request to validate connection
            await self.generate("Hello", max_tokens=1)
            return True
        except Exception as e:
            logger.error(f"Connection validation failed: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """
        Get information about the current model configuration.
        
        Returns:
            Dictionary containing model information
        """
        return {
            "model_name": self.model_name,
            "base_url": self.client.base_url,
            "max_retries": self.max_retries
        }-e \n# === src/config.py ===\n
# src/config.py

import os
import yaml
import logging
from pathlib import Path
from typing import List, Dict, Optional, Any, Union
from pydantic import BaseModel, Field, validator, root_validator

from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

logger = logging.getLogger(__name__)

class ConfigurationError(Exception):
    """Custom exception for configuration-related errors."""
    pass

class ExecutionStep(BaseModel):
    """Model for individual execution steps in the workflow."""
    module: str = Field(..., description="Name of the module to execute")
    dependencies: List[str] = Field(default_factory=list, description="List of required dependencies")
    parallel: bool = Field(default=False, description="Whether this step can run in parallel")
    timeout: Optional[int] = Field(default=None, description="Timeout in seconds for this step")
    
    @validator('module')
    def validate_module_name(cls, v):
        """Ensure module name is not empty and follows naming conventions."""
        if not v or not v.strip():
            raise ValueError("Module name cannot be empty")
        if not v.replace('_', '').replace('-', '').isalnum():
            raise ValueError("Module name must contain only alphanumeric characters, hyphens, and underscores")
        return v.strip()

class ModuleConfig(BaseModel):
    """Pydantic model for validating a module's configuration."""
    aspect: str = Field(..., description="The aspect or purpose of this module")
    instructions: str = Field(..., description="Instructions/prompt template for the module")
    llm_address: Optional[str] = Field(None, description="LLM provider address (e.g., 'openai:gpt-4')")
    api_key: Optional[str] = Field("local", description="API key or environment variable reference")
    fallback: Optional[str] = Field(None, description="Fallback model if primary fails")
    
    # Generation parameters
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="Sampling temperature")
    max_tokens: int = Field(default=1024, ge=1, le=8192, description="Maximum tokens to generate")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="Top-p sampling parameter")
    frequency_penalty: float = Field(default=0.0, ge=-2.0, le=2.0, description="Frequency penalty")
    presence_penalty: float = Field(default=0.0, ge=-2.0, le=2.0, description="Presence penalty")
    
    # Module-specific settings
    timeout: int = Field(default=30, ge=1, le=300, description="Request timeout in seconds")
    retries: int = Field(default=3, ge=0, le=10, description="Number of retry attempts")
    cache_enabled: bool = Field(default=True, description="Whether to use semantic caching")
    
    @validator('aspect')
    def validate_aspect(cls, v):
        """Ensure aspect is not empty."""
        if not v or not v.strip():
            raise ValueError("Aspect cannot be empty")
        return v.strip()
    
    @validator('instructions')
    def validate_instructions(cls, v):
        """Ensure instructions contain valid template variables."""
        if not v or not v.strip():
            raise ValueError("Instructions cannot be empty")
        
        # Check for basic template syntax issues
        if '{' in v and '}' in v:
            # This is a basic check - more sophisticated validation could be added
            try:
                # Test if template variables are properly formatted
                import re
                template_vars = re.findall(r'\{([^}]+)\}', v)
                for var in template_vars:
                    if not var.isidentifier():
                        logger.warning(f"Template variable '{var}' may not be a valid identifier")
            except Exception as e:
                logger.warning(f"Template validation warning: {e}")
        
        return v.strip()
    
    @validator('llm_address')
    def validate_llm_address(cls, v):
        """Validate LLM address format."""
        if v is None:
            return v
            
        v = v.strip()
        if not v:
            return None
            
        # Expected format: "provider:model" or just "provider"
        if ':' in v:
            provider, model = v.split(':', 1)
            if not provider or not model:
                raise ValueError("LLM address format should be 'provider:model'")
        else:
            # Just provider name
            if not v.replace('-', '').replace('_', '').isalnum():
                raise ValueError("Provider name should contain only alphanumeric characters, hyphens, and underscores")
        
        return v
    
    @validator('api_key', pre=True, always=True)
    def resolve_api_key_from_env(cls, v: Optional[str]) -> Optional[str]:
        """Resolve API key from environment variables."""
        if v and isinstance(v, str):
            v = v.strip()
            if v.startswith('${') and v.endswith('}'):
                var_name = v[2:-1]
                resolved_key = os.getenv(var_name)
                if not resolved_key:
                    raise ValueError(f"Environment variable '{var_name}' not found or empty")
                return resolved_key
            elif v.startswith('$'):
                # Handle $VAR_NAME format
                var_name = v[1:]
                resolved_key = os.getenv(var_name)
                if not resolved_key:
                    raise ValueError(f"Environment variable '{var_name}' not found or empty")
                return resolved_key
        return v
    
    def get_generation_params(self) -> Dict[str, Any]:
        """Get parameters for text generation."""
        return {
            'temperature': self.temperature,
            'max_tokens': self.max_tokens,
            'top_p': self.top_p,
            'frequency_penalty': self.frequency_penalty,
            'presence_penalty': self.presence_penalty
        }
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value with fallback."""
        return getattr(self, key, default)

class WorkflowConfig(BaseModel):
    """Pydantic model for validating the main workflow configuration."""
    name: str = Field(..., description="Name of the workflow")
    description: str = Field(..., description="Description of the workflow's purpose")
    version: str = Field(default="1.0.0", description="Workflow version")
    execution_plan: List[ExecutionStep] = Field(..., description="List of execution steps")
    
    # Global settings
    max_concurrent_steps: int = Field(default=5, ge=1, le=20, description="Maximum concurrent parallel steps")
    global_timeout: int = Field(default=300, ge=30, le=3600, description="Global workflow timeout in seconds")
    
    @validator('name')
    def validate_name(cls, v):
        """Validate workflow name."""
        if not v or not v.strip():
            raise ValueError("Workflow name cannot be empty")
        return v.strip()
    
    @validator('execution_plan')
    def validate_execution_plan(cls, v):
        """Validate execution plan consistency."""
        if not v:
            raise ValueError("Execution plan cannot be empty")
        
        # Check for duplicate module names
        module_names = [step.module for step in v]
        if len(module_names) != len(set(module_names)):
            duplicates = [name for name in set(module_names) if module_names.count(name) > 1]
            raise ValueError(f"Duplicate modules in execution plan: {duplicates}")
        
        # Validate dependencies exist in the plan
        all_modules = set(module_names)
        for step in v:
            for dep in step.dependencies:
                if dep not in all_modules:
                    raise ValueError(f"Module '{step.module}' depends on '{dep}' which is not in the execution plan")
        
        return v
    
    @root_validator
    def validate_dependency_cycles(cls, values):
        """Check for circular dependencies in the execution plan."""
        execution_plan = values.get('execution_plan', [])
        if not execution_plan:
            return values
        
        # Build dependency graph
        deps = {step.module: step.dependencies for step in execution_plan}
        
        # Check for cycles using DFS
        def has_cycle(node, visiting, visited):
            if node in visiting:
                return True
            if node in visited:
                return False
            
            visiting.add(node)
            for neighbor in deps.get(node, []):
                if has_cycle(neighbor, visiting, visited):
                    return True
            visiting.remove(node)
            visited.add(node)
            return False
        
        visited = set()
        for module in deps:
            if module not in visited:
                if has_cycle(module, set(), visited):
                    raise ValueError(f"Circular dependency detected involving module '{module}'")
        
        return values

class ConfigLoader:
    """Loads and validates all configurations for the orchestrator."""
    
    def __init__(self, config_path: Path):
        self.config_path = Path(config_path)
        self.workflow_config: Optional[WorkflowConfig] = None
        self.module_configs: Dict[str, ModuleConfig] = {}
        
        # Load and validate all configurations
        self._load_all_configs()
    
    def _load_all_configs(self) -> None:
        """Load and validate all configuration files."""
        try:
            logger.info(f"Loading configurations from: {self.config_path}")
            
            # Load workflow configuration
            self.workflow_config = self._load_workflow()
            logger.info(f"Loaded workflow: {self.workflow_config.name}")
            
            # Load module configurations
            self.module_configs = self._load_modules()
            logger.info(f"Loaded {len(self.module_configs)} module configurations")
            
            # Cross-validate
            self._validate_configuration_consistency()
            
            logger.info("Configuration loading completed successfully")
            
        except Exception as e:
            logger.error(f"Configuration loading failed: {e}")
            raise ConfigurationError(f"Failed to load configurations: {e}")
    
    def _load_yaml(self, file_path: Path) -> Dict[str, Any]:
        """Load and parse a YAML file safely."""
        try:
            if not file_path.exists():
                raise FileNotFoundError(f"Configuration file not found: {file_path}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = yaml.safe_load(f)
                
            if content is None:
                raise ValueError(f"YAML file is empty or invalid: {file_path}")
                
            return content
            
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML file {file_path}: {e}")
        except Exception as e:
            raise ConfigurationError(f"Error loading configuration file {file_path}: {e}")
    
    def _load_workflow(self) -> WorkflowConfig:
        """Load and validate the main workflow configuration."""
        workflow_path = self.config_path / "workflow.yaml"
        
        try:
            data = self._load_yaml(workflow_path)
            return WorkflowConfig(**data)
        except Exception as e:
            raise ConfigurationError(f"Failed to load workflow configuration: {e}")
    
    def _load_modules(self) -> Dict[str, ModuleConfig]:
        """Load and validate all module configurations."""
        modules_path = self.config_path / "modules"
        
        if not modules_path.exists():
            raise FileNotFoundError(f"Modules directory not found: {modules_path}")
        
        if not modules_path.is_dir():
            raise NotADirectoryError(f"Modules path is not a directory: {modules_path}")
        
        configs = {}
        yaml_files = list(modules_path.glob("*.yaml")) + list(modules_path.glob("*.yml"))
        
        if not yaml_files:
            logger.warning(f"No YAML files found in modules directory: {modules_path}")
            return configs
        
        for yaml_file in yaml_files:
            try:
                logger.debug(f"Loading module config: {yaml_file.name}")
                data = self._load_yaml(yaml_file)
                module_config = ModuleConfig(**data)
                
                # Use aspect as the key
                aspect = module_config.aspect
                if aspect in configs:
                    raise ValueError(f"Duplicate module aspect '{aspect}' found in file: {yaml_file.name}")
                
                configs[aspect] = module_config
                logger.debug(f"Successfully loaded module: {aspect}")
                
            except Exception as e:
                raise ConfigurationError(f"Failed to load module config from {yaml_file.name}: {e}")
        
        return configs
    
    def _validate_configuration_consistency(self) -> None:
        """Validate consistency between workflow and module configurations."""
        if not self.workflow_config:
            raise ConfigurationError("Workflow configuration not loaded")
        
        # Get all modules referenced in execution plan
        plan_modules = {step.module for step in self.workflow_config.execution_plan}
        loaded_modules = set(self.module_configs.keys())
        
        # Check for missing module configurations
        missing_modules = plan_modules - loaded_modules
        if missing_modules:
            raise ConfigurationError(
                f"Modules referenced in workflow but missing configuration files: {', '.join(sorted(missing_modules))}\n"
                f"Expected location: {self.config_path / 'modules'}/*.yaml"
            )
        
        # Check for unused module configurations (warning only)
        unused_modules = loaded_modules - plan_modules
        if unused_modules:
            logger.warning(f"Module configurations loaded but not used in workflow: {', '.join(sorted(unused_modules))}")
        
        # Validate that all modules have required dependencies available
        for step in self.workflow_config.execution_plan:
            for dep in step.dependencies:
                if dep not in loaded_modules:
                    raise ConfigurationError(f"Module '{step.module}' depends on '{dep}' which has no configuration")
    
    def get_module_config(self, module_name: str) -> Optional[ModuleConfig]:
        """Get configuration for a specific module."""
        return self.module_configs.get(module_name)
    
    def get_execution_order(self) -> List[List[str]]:
        """Get modules grouped by execution level (for parallel execution)."""
        if not self.workflow_config:
            return []
        
        # Topological sort with parallel execution support
        execution_levels = []
        remaining_modules = {step.module: step.dependencies for step in self.workflow_config.execution_plan}
        completed = set()
        
        while remaining_modules:
            # Find modules with no remaining dependencies
            ready_modules = []
            for module, deps in remaining_modules.items():
                if all(dep in completed for dep in deps):
                    ready_modules.append(module)
            
            if not ready_modules:
                # This should not happen due to cycle detection, but just in case
                raise ConfigurationError("Unable to resolve execution order - possible circular dependency")
            
            # Add ready modules to current execution level
            execution_levels.append(ready_modules)
            
            # Mark these modules as completed
            completed.update(ready_modules)
            
            # Remove completed modules from remaining
            for module in ready_modules:
                del remaining_modules[module]
        
        return execution_levels
    
    def reload(self) -> None:
        """Reload all configurations from disk."""
        logger.info("Reloading configurations...")
        self._load_all_configs()
    
    def get_config_summary(self) -> Dict[str, Any]:
        """Get a summary of loaded configurations."""
        if not self.workflow_config:
            return {"status": "not_loaded"}
        
        return {
            "workflow": {
                "name": self.workflow_config.name,
                "version": self.workflow_config.version,
                "steps": len(self.workflow_config.execution_plan),
                "max_concurrent": self.workflow_config.max_concurrent_steps
            },
            "modules": {
                "count": len(self.module_configs),
                "aspects": list(self.module_configs.keys()),
                "providers": list(set(
                    config.llm_address.split(':')[0] if config.llm_address and ':' in config.llm_address else 'local'
                    for config in self.module_configs.values()
                ))
            }
        }
    
    def validate_environment(self) -> Dict[str, Any]:
        """Validate that all required environment variables are available."""
        issues = []
        required_vars = set()
        
        # Collect all environment variables referenced in configurations
        for config in self.module_configs.values():
            if config.api_key and isinstance(config.api_key, str):
                if config.api_key.startswith('${') and config.api_key.endswith('}'):
                    var_name = config.api_key[2:-1]
                    required_vars.add(var_name)
                elif config.api_key.startswith('$'):
                    var_name = config.api_key[1:]
                    required_vars.add(var_name)
        
        # Check if variables are available
        missing_vars = []
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)
        
        if missing_vars:
            issues.append(f"Missing environment variables: {', '.join(missing_vars)}")
        
        return {
            "status": "valid" if not issues else "invalid",
            "issues": issues,
            "required_variables": list(required_vars),
            "missing_variables": missing_vars
        }-e \n# === src/main.py ===\n
#!/usr/bin/env python3
# src/main.py

import argparse
import asyncio
import logging
import signal
import sys
import time
from pathlib import Path
from typing import Optional

# Handle imports based on package structure
try:
    from .orchestrator import Orchestrator
    from .config import ConfigLoader, ConfigurationError
except ImportError:
    # Fallback for direct execution
    from orchestrator import Orchestrator
    from config import ConfigLoader, ConfigurationError

# Set up logging configuration
def setup_logging(level: str = "INFO", log_file: Optional[str] = None) -> None:
    """Configure application logging."""
    log_level = getattr(logging, level.upper(), logging.INFO)
    
    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(log_level)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level)
    console_handler.setFormatter(formatter)
    root_logger.addHandler(console_handler)
    
    # File handler (optional)
    if log_file:
        try:
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(log_level)
            file_handler.setFormatter(formatter)
            root_logger.addHandler(file_handler)
            logging.info(f"Logging to file: {log_file}")
        except Exception as e:
            logging.warning(f"Failed to set up file logging: {e}")

class ApplicationError(Exception):
    """Custom exception for application-level errors."""
    def __init__(self, message: str, exit_code: int = 1):
        super().__init__(message)
        self.exit_code = exit_code

class HybridLLMApp:
    """Main application class for the Hybrid LLM Orchestrator."""
    
    def __init__(self):
        self.orchestrator: Optional[Orchestrator] = None
        self.shutdown_requested = False
        self.logger = logging.getLogger(__name__)
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
    
    def _setup_signal_handlers(self):
        """Set up signal handlers for graceful shutdown."""
        def signal_handler(signum, frame):
            signal_name = signal.Signals(signum).name
            self.logger.info(f"Received {signal_name}, initiating graceful shutdown...")
            self.shutdown_requested = True
        
        try:
            signal.signal(signal.SIGINT, signal_handler)
            signal.signal(signal.SIGTERM, signal_handler)
        except (ValueError, OSError) as e:
            # Signal handling might not be available in all environments
            self.logger.warning(f"Could not set up signal handlers: {e}")
    
    def parse_arguments(self) -> argparse.Namespace:
        """Parse and validate command line arguments."""
        parser = argparse.ArgumentParser(
            description="Hybrid LLM Orchestrator - A production-grade AI workflow system",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Examples:
  %(prog)s --query "What are the ethical implications of AI?"
  %(prog)s --query "Analyze this data" --config-path /path/to/configs --verbose
  %(prog)s --health-check --config-path ./configs
            """
        )
        
        # Main operation arguments
        parser.add_argument(
            "--query",
            type=str,
            help="The user query to process through the workflow."
        )
        
        parser.add_argument(
            "--config-path",
            type=str,
            default="./configs",
            help="Path to the configuration directory (default: ./configs)"
        )
        
        # Operation modes
        parser.add_argument(
            "--health-check",
            action="store_true",
            help="Perform a health check and exit"
        )
        
        parser.add_argument(
            "--validate-config",
            action="store_true",
            help="Validate configuration files and exit"
        )
        
        parser.add_argument(
            "--config-summary",
            action="store_true",
            help="Show configuration summary and exit"
        )
        
        # Logging options
        parser.add_argument(
            "--log-level",
            choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
            default="INFO",
            help="Set the logging level (default: INFO)"
        )
        
        parser.add_argument(
            "--log-file",
            type=str,
            help="Path to log file (default: console only)"
        )
        
        parser.add_argument(
            "--verbose", "-v",
            action="store_true",
            help="Enable verbose output (equivalent to --log-level DEBUG)"
        )
        
        # Performance options
        parser.add_argument(
            "--timeout",
            type=int,
            default=300,
            help="Global timeout for workflow execution in seconds (default: 300)"
        )
        
        args = parser.parse_args()
        
        # Validate arguments
        self._validate_arguments(args)
        
        return args
    
    def _validate_arguments(self, args: argparse.Namespace) -> None:
        """Validate parsed arguments."""
        # Check if at least one operation is specified
        if not any([args.query, args.health_check, args.validate_config, args.config_summary]):
            raise ApplicationError("At least one operation must be specified (--query, --health-check, --validate-config, or --config-summary)", 2)
        
        # Validate query if provided
        if args.query and not args.query.strip():
            raise ApplicationError("Query cannot be empty", 2)
        
        # Validate config path
        config_path = Path(args.config_path)
        if not config_path.exists():
            raise ApplicationError(f"Configuration path does not exist: {config_path}", 2)
        
        if not config_path.is_dir():
            raise ApplicationError(f"Configuration path must be a directory: {config_path}", 2)
        
        # Validate timeout
        if args.timeout <= 0:
            raise ApplicationError("Timeout must be positive", 2)
        
        # Adjust log level for verbose mode
        if args.verbose:
            args.log_level = "DEBUG"
    
    async def validate_configuration(self, config_path: str) -> None:
        """Validate configuration files."""
        self.logger.info("Validating configuration files...")
        
        try:
            config_loader = ConfigLoader(Path(config_path))
            
            # Validate environment
            env_status = config_loader.validate_environment()
            if env_status["status"] != "valid":
                self.logger.error("Environment validation failed:")
                for issue in env_status["issues"]:
                    self.logger.error(f"  - {issue}")
                raise ApplicationError("Configuration validation failed", 3)
            
            # Show configuration summary
            summary = config_loader.get_config_summary()
            self.logger.info("Configuration validation successful:")
            self.logger.info(f"  Workflow: {summary['workflow']['name']} v{summary['workflow']['version']}")
            self.logger.info(f"  Modules: {summary['modules']['count']} loaded")
            self.logger.info(f"  Providers: {', '.join(summary['modules']['providers'])}")
            
        except ConfigurationError as e:
            self.logger.error(f"Configuration validation failed: {e}")
            raise ApplicationError(f"Invalid configuration: {e}", 3)
    
    async def show_config_summary(self, config_path: str) -> None:
        """Show detailed configuration summary."""
        try:
            config_loader = ConfigLoader(Path(config_path))
            summary = config_loader.get_config_summary()
            
            print("\n" + "="*60)
            print("CONFIGURATION SUMMARY")
            print("="*60)
            
            print(f"\nWorkflow Information:")
            print(f"  Name: {summary['workflow']['name']}")
            print(f"  Version: {summary['workflow']['version']}")
            print(f"  Steps: {summary['workflow']['steps']}")
            print(f"  Max Concurrent: {summary['workflow']['max_concurrent']}")
            
            print(f"\nModule Information:")
            print(f"  Total Modules: {summary['modules']['count']}")
            print(f"  Available Aspects: {', '.join(sorted(summary['modules']['aspects']))}")
            print(f"  LLM Providers: {', '.join(sorted(summary['modules']['providers']))}")
            
            # Environment status
            env_status = config_loader.validate_environment()
            print(f"\nEnvironment Status: {env_status['status'].upper()}")
            if env_status['required_variables']:
                print(f"  Required Variables: {', '.join(env_status['required_variables'])}")
            if env_status['missing_variables']:
                print(f"  Missing Variables: {', '.join(env_status['missing_variables'])}")
            
            print("="*60)
            
        except Exception as e:
            raise ApplicationError(f"Failed to show configuration summary: {e}", 3)
    
    async def perform_health_check(self, config_path: str) -> None:
        """Perform comprehensive system health check."""
        self.logger.info("Performing system health check...")
        
        start_time = time.time()
        
        try:
            # Initialize orchestrator for health check
            async with Orchestrator(config_path).managed_execution() as orchestrator:
                health_status = await orchestrator.health_check()
                
            check_duration = time.time() - start_time
            
            print("\n" + "="*60)
            print("SYSTEM HEALTH CHECK")
            print("="*60)
            
            print(f"\nOverall Status: {health_status['overall_status'].upper()}")
            print(f"Check Duration: {check_duration:.2f} seconds")
            
            # Database status
            db_status = health_status.get('components', {}).get('database', {})
            print(f"\nDatabase: {db_status.get('status', 'unknown').upper()}")
            if db_status.get('details'):
                print(f"  Details: {db_status['details']}")
            
            # External APIs
            api_status = health_status.get('components', {}).get('external_apis', {})
            print(f"\nExternal APIs:")
            for api, status in api_status.items():
                print(f"  {api}: {status.upper()}")
            
            # Circuit breakers
            breaker_status = health_status.get('components', {}).get('circuit_breakers', {})
            if breaker_status:
                print(f"\nCircuit Breakers:")
                for breaker, info in breaker_status.items():
                    state = info.get('state', 'unknown') if isinstance(info, dict) else str(info)
                    print(f"  {breaker}: {state}")
            
            # Clients
            client_status = health_status.get('components', {}).get('clients', {})
            if client_status:
                print(f"\nLLM Clients:")
                for client, status in client_status.items():
                    print(f"  {client}: {status.upper()}")
            
            # System features
            components = health_status.get('components', {})
            print(f"\nSystem Features:")
            print(f"  Semantic Cache: {components.get('cache', 'unknown')}")
            print(f"  Router: {components.get('router', 'unknown')}")
            
            print("="*60)
            
            if health_status['overall_status'] != 'healthy':
                raise ApplicationError("System health check failed", 4)
                
        except Exception as e:
            if isinstance(e, ApplicationError):
                raise
            raise ApplicationError(f"Health check failed: {e}", 4)
    
    async def process_query(self, query: str, config_path: str, timeout: int) -> str:
        """Process user query through the workflow."""
        self.logger.info(f"Processing query: {query[:100]}{'...' if len(query) > 100 else ''}")
        
        start_time = time.time()
        
        try:
            # Initialize orchestrator with timeout
            async with asyncio.timeout(timeout):
                async with Orchestrator(config_path).managed_execution() as orchestrator:
                    self.orchestrator = orchestrator
                    
                    # Check if shutdown was requested
                    if self.shutdown_requested:
                        raise ApplicationError("Shutdown requested before processing", 130)
                    
                    # Process the query
                    result = await orchestrator.execute_workflow(query)
                    
            processing_time = time.time() - start_time
            self.logger.info(f"Query processed successfully in {processing_time:.2f} seconds")
            
            return result
            
        except asyncio.TimeoutError:
            raise ApplicationError(f"Query processing timed out after {timeout} seconds", 5)
        except KeyboardInterrupt:
            raise ApplicationError("Processing interrupted by user", 130)
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"Query processing failed after {processing_time:.2f} seconds: {e}")
            raise ApplicationError(f"Query processing failed: {e}", 6)
    
    def display_result(self, result: str) -> None:
        """Display the final result with formatting."""
        print("\n" + "="*20 + " FINAL RESPONSE " + "="*20)
        print(result)
        print("="*58)
    
    async def run(self) -> int:
        """Main application run method."""
        try:
            # Parse arguments
            args = self.parse_arguments()
            
            # Set up logging
            setup_logging(args.log_level, args.log_file)
            
            self.logger.info("Starting Hybrid LLM Orchestrator")
            self.logger.debug(f"Arguments: {vars(args)}")
            
            # Handle different operation modes
            if args.validate_config:
                await self.validate_configuration(args.config_path)
                print(" Configuration validation successful")
                return 0
            
            elif args.config_summary:
                await self.show_config_summary(args.config_path)
                return 0
            
            elif args.health_check:
                await self.perform_health_check(args.config_path)
                print(" System health check passed")
                return 0
            
            elif args.query:
                result = await self.process_query(args.query, args.config_path, args.timeout)
                self.display_result(result)
                return 0
            
            else:
                # This should not happen due to argument validation
                raise ApplicationError("No operation specified", 2)
                
        except ApplicationError as e:
            self.logger.error(f"Application error: {e}")
            return e.exit_code
            
        except KeyboardInterrupt:
            self.logger.info("Application interrupted by user")
            return 130
            
        except Exception as e:
            self.logger.exception(f"Unexpected error: {e}")
            return 1
        
        finally:
            if self.orchestrator:
                try:
                    await self.orchestrator.cleanup()
                except Exception as e:
                    self.logger.warning(f"Error during cleanup: {e}")
            
            self.logger.info("Hybrid LLM Orchestrator shutdown complete")

async def main():
    """Async main entry point."""
    app = HybridLLMApp()
    exit_code = await app.run()
    sys.exit(exit_code)

def sync_main():
    """Synchronous entry point for setuptools."""
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        sys.exit(130)
    except Exception as e:
        logging.error(f"Failed to start application: {e}")
        sys.exit(1)

if __name__ == "__main__":
    sync_main()-e \n# === src/orchestrator.py ===\n
# src/orchestrator.py

import asyncio
import sqlite3
import aiohttp
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List
from contextlib import asynccontextmanager

from .config import ConfigLoader, ModuleConfig
from .utils.security import SecurityManager
from .clients.base import LlmClient
from .clients.openai_compatible import OpenAICompatibleClient
from .utils.local_inference import BatchedLocalInference, OptimizedLocalModelLoader

# External dependencies
from routellm.controller import Controller
from upstash_semantic_cache import SemanticCache
from pybreaker import CircuitBreaker, CircuitBreakerError

logger = logging.getLogger(__name__)

class OrchestrationError(Exception):
    """Custom exception for orchestration errors."""
    pass

class Orchestrator:
    """Manages the execution of the AI workflow with production-grade features."""

    def __init__(self, config_path: str, db_path: str = "users.db"):
        self.db_path = db_path
        self.config_loader = ConfigLoader(Path(config_path))
        self.security_manager = SecurityManager()
        
        # Client registry for different LLM providers
        self.clients: Dict[str, LlmClient] = {}
        self.local_inference: Optional[BatchedLocalInference] = None
        
        # Production features
        self.router: Optional[Controller] = None
        self.semantic_cache: Optional[SemanticCache] = None
        self.circuit_breakers: Dict[str, CircuitBreaker] = {}
        
        # State management
        self._initialized = False
        self._session: Optional[aiohttp.ClientSession] = None

    async def initialize(self) -> None:
        """Initialize all components asynchronously."""
        if self._initialized:
            return
            
        try:
            logger.info("Initializing Orchestrator...")
            
            # Initialize HTTP session
            self._session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))
            
            # Initialize components
            await self._initialize_router()
            await self._initialize_cache()
            await self._initialize_circuit_breakers()
            await self._initialize_clients()
            
            self._initialized = True
            logger.info("Orchestrator initialization complete")
            
        except Exception as e:
            logger.error(f"Failed to initialize Orchestrator: {e}")
            await self.cleanup()
            raise OrchestrationError(f"Initialization failed: {e}")

    async def _initialize_router(self) -> None:
        """Initialize the RouteLLM controller."""
        try:
            # RouteLLM initialization - this may be sync, so we run in executor
            self.router = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: Controller(
                    routers=["mf"],  # Matrix Factorization router
                    strong_model="gpt-4o",
                    weak_model="huggingface:google/gemma-2b-it"
                )
            )
            logger.info("RouteLLM controller initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize router: {e}. Routing will be disabled.")
            self.router = None

    async def _initialize_cache(self) -> None:
        """Initialize semantic cache."""
        try:
            self.semantic_cache = SemanticCache(min_proximity=0.90)
            logger.info("Semantic cache initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize cache: {e}. Caching will be disabled.")
            self.semantic_cache = None

    async def _initialize_circuit_breakers(self) -> None:
        """Initialize circuit breakers for external services."""
        # Get unique external endpoints from config
        external_endpoints = self._get_external_endpoints()
        
        for endpoint in external_endpoints:
            self.circuit_breakers[endpoint] = CircuitBreaker(
                fail_max=3,
                reset_timeout=60,
                recovery_timeout=30,
                expected_exception=Exception
            )
        
        logger.info(f"Initialized circuit breakers for {len(self.circuit_breakers)} endpoints")

    def _get_external_endpoints(self) -> List[str]:
        """Extract external endpoints from configuration."""
        endpoints = set()
        
        for module_config in self.config_loader.module_configs.values():
            if hasattr(module_config, 'llm_address'):
                # Extract provider from address (e.g., "openai:gpt-4o" -> "openai")
                address = module_config.llm_address
                if ":" in address and not address.startswith("huggingface"):
                    provider = address.split(":")[0]
                    endpoints.add(provider)
                    
        return list(endpoints)

    async def _initialize_clients(self) -> None:
        """Initialize LLM clients for different providers."""
        for module_name, config in self.config_loader.module_configs.items():
            if hasattr(config, 'llm_address'):
                client = await self._create_client(config)
                if client:
                    self.clients[module_name] = client

    async def _create_client(self, config: ModuleConfig) -> Optional[LlmClient]:
        """Create appropriate client based on configuration."""
        try:
            address = config.llm_address
            
            if address.startswith("openai:"):
                return OpenAICompatibleClient(
                    api_key=config.get("api_key", ""),
                    base_url="https://api.openai.com/v1"
                )
            elif address.startswith("anthropic:"):
                return OpenAICompatibleClient(
                    api_key=config.get("api_key", ""),
                    base_url="https://api.anthropic.com/v1"
                )
            elif address.startswith("huggingface:"):
                # For local models, initialize if not already done
                if not self.local_inference:
                    model_name = address.split(":", 1)[1]
                    model, tokenizer = OptimizedLocalModelLoader.load_model_and_tokenizer(model_name)
                    self.local_inference = BatchedLocalInference(model, tokenizer)
                    await self.local_inference.start()
                return self.local_inference
            else:
                logger.warning(f"Unknown provider in address: {address}")
                return None
                
        except Exception as e:
            logger.error(f"Failed to create client for {config.llm_address}: {e}")
            return None

    async def _execute_module(self, module_name: str, context: Dict[str, Any]) -> str:
        """Execute a single module with security, caching, routing, and resilience."""
        config = self.config_loader.module_configs.get(module_name)
        if not config:
            raise OrchestrationError(f"Module '{module_name}' not found in configuration")

        # Format prompt with context
        try:
            prompt = config.instructions.format(**context)
        except KeyError as e:
            raise OrchestrationError(f"Missing context variable for module '{module_name}': {e}")

        # 1. Security: Prompt Injection Check
        try:
            is_injection, confidence = self.security_manager.detect_injection(prompt)
            if is_injection and confidence > 0.8:  # High confidence threshold
                logger.warning(f"Potential prompt injection detected in module '{module_name}' (confidence: {confidence})")
                return "Error: Potentially unsafe input detected."
        except Exception as e:
            logger.warning(f"Security check failed for module '{module_name}': {e}")

        # 2. Caching: Check Semantic Cache
        if self.semantic_cache:
            try:
                cached_result = self.semantic_cache.get(prompt)
                if cached_result:
                    logger.info(f"Cache hit for module '{module_name}'")
                    return cached_result
            except Exception as e:
                logger.warning(f"Cache lookup failed: {e}")

        logger.info(f"Executing module '{module_name}'...")

        # 3. PII Scrubbing
        try:
            scrubbed_prompt = self.security_manager.scrub_pii(prompt)
        except Exception as e:
            logger.warning(f"PII scrubbing failed: {e}")
            scrubbed_prompt = prompt

        # 4. Execution with resilience
        result = await self._execute_with_resilience(module_name, scrubbed_prompt, config)

        # 5. Store in cache
        if self.semantic_cache and result:
            try:
                self.semantic_cache.set(prompt, result)
            except Exception as e:
                logger.warning(f"Cache storage failed: {e}")

        return result

    async def _execute_with_resilience(self, module_name: str, prompt: str, config: ModuleConfig) -> str:
        """Execute module with circuit breaker protection."""
        # Get appropriate client
        client = self.clients.get(module_name)
        if not client:
            return f"Error: No client available for module '{module_name}'"

        # Determine if we need circuit breaker protection
        provider = self._get_provider_from_config(config)
        circuit_breaker = self.circuit_breakers.get(provider) if provider else None

        try:
            if circuit_breaker:
                # Wrap execution in circuit breaker
                result = await self._execute_with_circuit_breaker(
                    circuit_breaker, client, prompt, **config.get_generation_params()
                )
            else:
                # Direct execution for local models
                result = await client.generate(prompt, **config.get_generation_params())

            return result

        except CircuitBreakerError:
            logger.warning(f"Circuit breaker open for module '{module_name}', using fallback")
            return await self._fallback_execution(prompt)
        except Exception as e:
            logger.error(f"Module '{module_name}' execution failed: {e}")
            return f"Error: Module {module_name} failed to produce a result."

    async def _execute_with_circuit_breaker(self, breaker: CircuitBreaker, client: LlmClient, prompt: str, **kwargs) -> str:
        """Execute client call wrapped in circuit breaker."""
        def sync_call():
            # This is a workaround for circuit breakers that expect sync functions
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(client.generate(prompt, **kwargs))
            finally:
                loop.close()

        # Run the circuit-breaker-protected call in executor
        return await asyncio.get_event_loop().run_in_executor(None, breaker(sync_call))

    async def _fallback_execution(self, prompt: str) -> str:
        """Fallback execution when primary service is unavailable."""
        if self.local_inference:
            try:
                return await self.local_inference.generate(prompt)
            except Exception as e:
                logger.error(f"Fallback execution failed: {e}")
        
        return "Error: Service temporarily unavailable. Please try again later."

    def _get_provider_from_config(self, config: ModuleConfig) -> Optional[str]:
        """Extract provider name from module configuration."""
        if hasattr(config, 'llm_address'):
            address = config.llm_address
            if ":" in address and not address.startswith("huggingface"):
                return address.split(":")[0]
        return None

    async def execute_workflow(self, query: str) -> str:
        """Execute the full workflow defined in configuration."""
        if not self._initialized:
            await self.initialize()

        execution_plan = self.config_loader.workflow_config.execution_plan
        context = {"query": query}

        try:
            # Execute modules according to plan
            for step in execution_plan:
                module_name = step["module"]
                dependencies = step.get("dependencies", [])

                # Check dependencies
                missing_deps = [dep for dep in dependencies if dep not in context]
                if missing_deps:
                    raise OrchestrationError(f"Missing dependencies for module {module_name}: {missing_deps}")

                logger.info(f"Running module: {module_name}")
                result = await self._execute_module(module_name, context)
                
                # Store results with multiple keys for flexibility
                context[module_name] = result
                context[f"{module_name}_output"] = result

            # Final synthesis
            logger.info("Synthesizing final response")
            final_response = await self._synthesize_response(query, context)
            
            return final_response

        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            raise OrchestrationError(f"Workflow execution failed: {e}")

    async def _synthesize_response(self, query: str, context: Dict[str, Any]) -> str:
        """Synthesize final response from module outputs."""
        synthesis_prompt = self._build_synthesis_prompt(query, context)
        
        # Use local model for synthesis to reduce costs and latency
        if self.local_inference:
            try:
                return await self.local_inference.generate(synthesis_prompt)
            except Exception as e:
                logger.warning(f"Local synthesis failed: {e}")

        # Fallback to any available client
        for client in self.clients.values():
            try:
                return await client.generate(synthesis_prompt, max_tokens=512)
            except Exception as e:
                logger.warning(f"Synthesis fallback failed: {e}")
                continue

        return "Error: Unable to synthesize final response."

    def _build_synthesis_prompt(self, query: str, context: Dict[str, Any]) -> str:
        """Build the synthesis prompt from context."""
        prompt_parts = [
            f"Synthesize the following information into a coherent response for: \"{query}\"",
            ""
        ]

        # Add module outputs
        for key, value in context.items():
            if key.endswith("_output") and value:
                module_name = key.replace("_output", "")
                prompt_parts.append(f"{module_name} Output:")
                prompt_parts.append(str(value))
                prompt_parts.append("")

        prompt_parts.append("Final Response:")
        return "\n".join(prompt_parts)

    async def health_check(self) -> Dict[str, Any]:
        """Perform comprehensive health check of all system components."""
        if not self._initialized:
            return {"overall_status": "not_initialized"}

        # Check database
        db_status = await self._check_db_connection()
        
        # Check external APIs
        api_status = await self._check_api_connectivity()
        
        # Check circuit breakers
        breaker_status = {
            name: {
                "state": str(cb.current_state),
                "failure_count": cb.fail_counter,
                "last_failure_time": getattr(cb, 'last_failure_time', None)
            }
            for name, cb in self.circuit_breakers.items()
        }

        # Check clients
        client_status = {}
        for name, client in self.clients.items():
            try:
                if hasattr(client, 'validate_connection'):
                    client_status[name] = "ok" if await client.validate_connection() else "error"
                else:
                    client_status[name] = "unknown"
            except Exception:
                client_status[name] = "error"

        overall_healthy = (
            db_status["status"] == "ok" and
            all(status == "ok" for status in api_status.values()) and
            all(status in ["ok", "unknown"] for status in client_status.values())
        )

        return {
            "overall_status": "healthy" if overall_healthy else "degraded",
            "timestamp": asyncio.get_event_loop().time(),
            "components": {
                "database": db_status,
                "external_apis": api_status,
                "circuit_breakers": breaker_status,
                "clients": client_status,
                "cache": "enabled" if self.semantic_cache else "disabled",
                "router": "enabled" if self.router else "disabled"
            }
        }

    async def _check_db_connection(self) -> Dict[str, Any]:
        """Check database connectivity."""
        try:
            def db_check():
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("SELECT 1").fetchone()
                    
            await asyncio.get_event_loop().run_in_executor(None, db_check)
            return {"status": "ok"}
        except Exception as e:
            return {"status": "error", "details": str(e)}

    async def _check_api_connectivity(self) -> Dict[str, Any]:
        """Check connectivity to external APIs."""
        endpoints = {
            "OpenAI": "https://api.openai.com/v1/models",
            "Anthropic": "https://api.anthropic.com/v1/messages",
            "xAI": "https://api.x.ai/v1/models"
        }
        
        statuses = {}
        
        if not self._session:
            return {name: "session_not_available" for name in endpoints}

        for name, url in endpoints.items():
            try:
                async with self._session.head(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
                    statuses[name] = "ok" if response.status < 500 else "error"
            except asyncio.TimeoutError:
                statuses[name] = "timeout"
            except Exception:
                statuses[name] = "unreachable"
                
        return statuses

    @asynccontextmanager
    async def managed_execution(self):
        """Context manager for automatic initialization and cleanup."""
        try:
            await self.initialize()
            yield self
        finally:
            await self.cleanup()

    async def cleanup(self) -> None:
        """Clean up all resources."""
        logger.info("Cleaning up Orchestrator resources...")
        
        # Close HTTP session
        if self._session and not self._session.closed:
            await self._session.close()
            
        # Cleanup local inference
        if self.local_inference:
            try:
                await self.local_inference.stop()
            except Exception as e:
                logger.warning(f"Error stopping local inference: {e}")

        # Reset state
        self._initialized = False
        logger.info("Orchestrator cleanup complete")

    def get_stats(self) -> Dict[str, Any]:
        """Get current orchestrator statistics."""
        return {
            "initialized": self._initialized,
            "modules_configured": len(self.config_loader.module_configs) if hasattr(self.config_loader, 'module_configs') else 0,
            "clients_active": len(self.clients),
            "circuit_breakers": len(self.circuit_breakers),
            "cache_enabled": self.semantic_cache is not None,
            "router_enabled": self.router is not None,
            "local_inference": self.local_inference is not None
        }-e \n# === src/utils/caching.py ===\n
# src/utils/caching.py

import asyncio
import hashlib
import logging
import time
from pathlib import Path
from typing import Optional, Dict, Any, Union
import diskcache as dc

logger = logging.getLogger(__name__)

class CacheError(Exception):
    """Custom exception for cache-related errors."""
    pass

class Cache:
    """
    A persistent on-disk cache for LLM responses with expiration and size limits.
    
    Features:
    - SHA256 hashing for keys
    - Configurable expiration times
    - Size limits with LRU eviction
    - Async-safe operations
    - Statistics tracking
    """

    def __init__(
        self, 
        cache_dir: str = ".cache",
        max_size: int = 1024 * 1024 * 1024,  # 1GB default
        default_expire: int = 86400,  # 24 hours default
        eviction_policy: str = "least-recently-used"
    ):
        """
        Initialize the cache.
        
        Args:
            cache_dir: Directory to store cache files
            max_size: Maximum cache size in bytes
            default_expire: Default expiration time in seconds
            eviction_policy: Cache eviction policy
        """
        self.cache_dir = Path(cache_dir)
        self.default_expire = default_expire
        self.stats = {
            "hits": 0,
            "misses": 0,
            "errors": 0,
            "sets": 0
        }
        
        try:
            # Create cache directory
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            
            # Initialize diskcache with configuration
            self.cache = dc.Cache(
                directory=str(self.cache_dir),
                size_limit=max_size,
                eviction_policy=eviction_policy
            )
            
            logger.info(f"Cache initialized at {self.cache_dir} (max_size: {max_size} bytes)")
            
        except Exception as e:
            raise CacheError(f"Failed to initialize cache: {e}")

    def _generate_key(self, text: str, prefix: str = "") -> str:
        """
        Create a SHA256 hash of the input text to use as a cache key.
        
        Args:
            text: Input text to hash
            prefix: Optional prefix for the key
            
        Returns:
            SHA256 hash as hex string
        """
        try:
            key_input = f"{prefix}{text}".encode('utf-8')
            return hashlib.sha256(key_input).hexdigest()
        except Exception as e:
            logger.error(f"Failed to generate cache key: {e}")
            raise CacheError(f"Key generation failed: {e}")

    async def get(self, key_text: str, prefix: str = "") -> Optional[str]:
        """
        Retrieve an item from the cache asynchronously.
        
        Args:
            key_text: Text to use for key generation
            prefix: Optional prefix for cache key
            
        Returns:
            Cached value if found, None otherwise
        """
        try:
            key = self._generate_key(key_text, prefix)
            
            # Run cache operation in thread pool to avoid blocking
            result = await asyncio.get_event_loop().run_in_executor(
                None, self._sync_get, key
            )
            
            if result is not None:
                self.stats["hits"] += 1
                logger.debug(f"Cache hit for key: {key[:16]}...")
                return result
            else:
                self.stats["misses"] += 1
                logger.debug(f"Cache miss for key: {key[:16]}...")
                return None
                
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Cache get error: {e}")
            return None

    def _sync_get(self, key: str) -> Optional[str]:
        """Synchronous cache get operation."""
        try:
            return self.cache.get(key)
        except Exception as e:
            logger.error(f"Disk cache get error: {e}")
            return None

    async def set(
        self, 
        key_text: str, 
        value: str, 
        expire: Optional[int] = None,
        prefix: str = ""
    ) -> bool:
        """
        Save an item to the cache asynchronously.
        
        Args:
            key_text: Text to use for key generation
            value: Value to cache
            expire: Expiration time in seconds (None for default)
            prefix: Optional prefix for cache key
            
        Returns:
            True if successfully cached, False otherwise
        """
        try:
            key = self._generate_key(key_text, prefix)
            expire_time = expire or self.default_expire
            
            # Run cache operation in thread pool
            success = await asyncio.get_event_loop().run_in_executor(
                None, self._sync_set, key, value, expire_time
            )
            
            if success:
                self.stats["sets"] += 1
                logger.debug(f"Cache set for key: {key[:16]}... (expire: {expire_time}s)")
            else:
                self.stats["errors"] += 1
                
            return success
            
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Cache set error: {e}")
            return False

    def _sync_set(self, key: str, value: str, expire: int) -> bool:
        """Synchronous cache set operation."""
        try:
            return self.cache.set(key, value, expire=expire)
        except Exception as e:
            logger.error(f"Disk cache set error: {e}")
            return False

    async def delete(self, key_text: str, prefix: str = "") -> bool:
        """
        Delete an item from the cache.
        
        Args:
            key_text: Text to use for key generation
            prefix: Optional prefix for cache key
            
        Returns:
            True if item was deleted, False otherwise
        """
        try:
            key = self._generate_key(key_text, prefix)
            
            success = await asyncio.get_event_loop().run_in_executor(
                None, self._sync_delete, key
            )
            
            if success:
                logger.debug(f"Cache delete for key: {key[:16]}...")
            
            return success
            
        except Exception as e:
            logger.error(f"Cache delete error: {e}")
            return False

    def _sync_delete(self, key: str) -> bool:
        """Synchronous cache delete operation."""
        try:
            return self.cache.delete(key)
        except Exception as e:
            logger.error(f"Disk cache delete error: {e}")
            return False

    async def clear(self) -> bool:
        """
        Clear all items from the cache.
        
        Returns:
            True if cache was cleared successfully, False otherwise
        """
        try:
            await asyncio.get_event_loop().run_in_executor(None, self.cache.clear)
            logger.info("Cache cleared successfully")
            
            # Reset statistics
            self.stats = {
                "hits": 0,
                "misses": 0,
                "errors": 0,
                "sets": 0
            }
            
            return True
            
        except Exception as e:
            logger.error(f"Cache clear error: {e}")
            return False

    def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics and information.
        
        Returns:
            Dictionary containing cache statistics
        """
        try:
            # Get cache volume information
            volume_info = dict(self.cache.volume())
            
            # Calculate hit rate
            total_requests = self.stats["hits"] + self.stats["misses"]
            hit_rate = (self.stats["hits"] / total_requests * 100) if total_requests > 0 else 0
            
            return {
                "statistics": {
                    **self.stats,
                    "hit_rate_percent": round(hit_rate, 2),
                    "total_requests": total_requests
                },
                "volume": volume_info,
                "configuration": {
                    "cache_dir": str(self.cache_dir),
                    "default_expire": self.default_expire,
                    "max_size": getattr(self.cache, '_size_limit', 'unknown')
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get cache stats: {e}")
            return {
                "statistics": self.stats,
                "error": str(e)
            }

    async def cleanup_expired(self) -> int:
        """
        Clean up expired cache entries.
        
        Returns:
            Number of entries cleaned up
        """
        try:
            # Note: diskcache handles expiration automatically,
            # but we can trigger a manual cleanup
            cleaned = await asyncio.get_event_loop().run_in_executor(
                None, self._sync_cleanup
            )
            
            if cleaned > 0:
                logger.info(f"Cleaned up {cleaned} expired cache entries")
            
            return cleaned
            
        except Exception as e:
            logger.error(f"Cache cleanup error: {e}")
            return 0

    def _sync_cleanup(self) -> int:
        """Synchronous cache cleanup operation."""
        try:
            # Get current size before cleanup
            initial_count = len(self.cache)
            
            # Trigger eviction and cleanup
            self.cache.expire()
            
            # Calculate cleaned entries
            final_count = len(self.cache)
            return max(0, initial_count - final_count)
            
        except Exception as e:
            logger.error(f"Disk cache cleanup error: {e}")
            return 0

    def __len__(self) -> int:
        """Get number of items in cache."""
        try:
            return len(self.cache)
        except Exception:
            return 0

    def close(self) -> None:
        """Close the cache and clean up resources."""
        try:
            self.cache.close()
            logger.info("Cache closed successfully")
        except Exception as e:
            logger.error(f"Error closing cache: {e}")


class SemanticCache(Cache):
    """
    Extension of Cache that provides semantic similarity-based caching.
    
    This would require a vector similarity backend for production use.
    For now, it's a placeholder that uses exact text matching.
    """
    
    def __init__(self, *args, similarity_threshold: float = 0.9, **kwargs):
        super().__init__(*args, **kwargs)
        self.similarity_threshold = similarity_threshold
        logger.warning("SemanticCache is using exact text matching. For production, implement vector similarity.")

    async def get_similar(self, key_text: str, prefix: str = "") -> Optional[str]:
        """
        Get cached value for semantically similar text.
        
        Currently implements exact matching as placeholder.
        In production, this would use vector similarity search.
        """
        # For now, fall back to exact matching
        return await self.get(key_text, prefix)

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics including semantic cache info."""
        stats = super().get_stats()
        stats["semantic_config"] = {
            "similarity_threshold": self.similarity_threshold,
            "implementation": "exact_match_placeholder"
        }
        return stats-e \n# === src/utils/local_inference.py ===\n
# src/utils/local_inference.py

import torch
import asyncio
import logging
from asyncio import Queue
from typing import List, Dict, Any, Optional, Tuple
from contextlib import asynccontextmanager
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    BitsAndBytesConfig,
    GenerationConfig
)

logger = logging.getLogger(__name__)

class OptimizedLocalModelLoader:
    """Loads Hugging Face models with optimizations for memory efficiency."""
    
    @staticmethod
    def _get_optimal_dtype() -> torch.dtype:
        """Determine the optimal dtype based on hardware capabilities."""
        if torch.cuda.is_available():
            # Check if bfloat16 is supported
            if torch.cuda.get_device_capability()[0] >= 8:  # Ampere or newer
                return torch.bfloat16
            else:
                return torch.float16
        return torch.float32

    @staticmethod
    def _create_quantization_config() -> Optional[BitsAndBytesConfig]:
        """Create quantization config if CUDA is available."""
        if not torch.cuda.is_available():
            logger.warning("CUDA not available, skipping quantization")
            return None
            
        return BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=OptimizedLocalModelLoader._get_optimal_dtype(),
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_storage=torch.uint8
        )

    @classmethod
    def load_model_and_tokenizer(
        self, 
        model_name: str,
        use_quantization: bool = True,
        trust_remote_code: bool = False
    ) -> Tuple[Any, Any]:
        """
        Load model and tokenizer with optimizations.
        
        Args:
            model_name: HuggingFace model identifier
            use_quantization: Whether to use 4-bit quantization
            trust_remote_code: Whether to trust remote code execution
            
        Returns:
            Tuple of (model, tokenizer)
        """
        if not model_name:
            raise ValueError("Model name cannot be empty")

        try:
            # Load tokenizer first
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=trust_remote_code
            )
            
            # Set padding token if not present
            if tokenizer.pad_token is None:
                if tokenizer.eos_token is not None:
                    tokenizer.pad_token = tokenizer.eos_token
                else:
                    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            
            # Configure padding for batched inference
            tokenizer.padding_side = "left"  # Important for generation

            # Prepare model loading arguments
            model_kwargs = {
                "torch_dtype": OptimizedLocalModelLoader._get_optimal_dtype(),
                "trust_remote_code": trust_remote_code,
                "low_cpu_mem_usage": True
            }

            # Add quantization if requested and available
            if use_quantization and torch.cuda.is_available():
                model_kwargs["quantization_config"] = OptimizedLocalModelLoader._create_quantization_config()
                model_kwargs["device_map"] = "auto"
            elif torch.cuda.is_available():
                model_kwargs["device_map"] = "auto"

            # Load model
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                **model_kwargs
            )
            
            # Resize token embeddings if we added new tokens
            if len(tokenizer) > model.config.vocab_size:
                model.resize_token_embeddings(len(tokenizer))

            logger.info(f"Successfully loaded model '{model_name}' with device map: {getattr(model, 'hf_device_map', 'single device')}")
            return model, tokenizer

        except Exception as e:
            logger.error(f"Failed to load model '{model_name}': {e}")
            raise

class BatchedLocalInference:
    """A server that batches incoming requests for higher throughput."""
    
    def __init__(
        self, 
        model: Any, 
        tokenizer: Any, 
        max_batch_size: int = 8,
        timeout_ms: int = 50,
        max_new_tokens: int = 512,
        generation_config: Optional[Dict[str, Any]] = None
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.queue: Queue = Queue()
        self.max_batch_size = max_batch_size
        self.timeout = timeout_ms / 1000.0
        self.max_new_tokens = max_new_tokens
        self._running = False
        self._runner_task: Optional[asyncio.Task] = None
        
        # Setup generation config
        self.generation_config = GenerationConfig(
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            **(generation_config or {})
        )

    async def start(self) -> None:
        """Start the batch processor."""
        if self._running:
            logger.warning("Batch processor is already running")
            return
            
        self._running = True
        self._runner_task = asyncio.create_task(self._batch_processor())
        logger.info("Batch processor started")

    async def stop(self) -> None:
        """Stop the batch processor and cleanup resources."""
        self._running = False
        
        if self._runner_task and not self._runner_task.done():
            try:
                await asyncio.wait_for(self._runner_task, timeout=5.0)
            except asyncio.TimeoutError:
                logger.warning("Batch processor did not stop gracefully, cancelling...")
                self._runner_task.cancel()
                try:
                    await self._runner_task
                except asyncio.CancelledError:
                    pass
        
        logger.info("Batch processor stopped")

    @asynccontextmanager
    async def managed_inference(self):
        """Context manager for automatic start/stop of the inference server."""
        await self.start()
        try:
            yield self
        finally:
            await self.stop()

    async def _batch_processor(self) -> None:
        """Continuously processes batches of requests from the queue."""
        logger.info("Batch processor loop started")
        
        while self._running:
            try:
                batch: List[Dict[str, Any]] = []
                start_time = asyncio.get_event_loop().time()
                
                # Collect batch within timeout
                while len(batch) < self.max_batch_size and self._running:
                    try:
                        remaining_time = max(0, self.timeout - (asyncio.get_event_loop().time() - start_time))
                        if remaining_time <= 0:
                            break
                            
                        request = await asyncio.wait_for(
                            self.queue.get(), 
                            timeout=remaining_time
                        )
                        batch.append(request)
                        
                    except asyncio.TimeoutError:
                        break
                
                # Process batch if we have requests
                if batch:
                    await self._process_batch(batch)
                    
            except Exception as e:
                logger.error(f"Error in batch processor: {e}")
                # Continue processing other batches
                await asyncio.sleep(0.1)

    async def _process_batch(self, batch: List[Dict[str, Any]]) -> None:
        """Process a batch of generation requests."""
        try:
            prompts = [req['prompt'] for req in batch]
            
            # Tokenize batch
            inputs = self.tokenizer(
                prompts, 
                return_tensors="pt", 
                padding=True,
                truncation=True,
                max_length=2048  # Reasonable max length
            )
            
            # Move to model device
            inputs = inputs.to(self.model.device)
            
            # Generate responses
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    generation_config=self.generation_config,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # Decode results (remove input tokens from output)
            input_lengths = inputs['attention_mask'].sum(dim=1)
            results = []
            
            for i, output in enumerate(outputs):
                # Extract only the newly generated tokens
                generated_tokens = output[input_lengths[i]:]
                decoded = self.tokenizer.decode(
                    generated_tokens, 
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True
                )
                results.append(decoded.strip())
            
            # Set results for all requests in the batch
            for req, result in zip(batch, results):
                if not req['future'].cancelled():
                    req['future'].set_result(result)
                    
        except Exception as e:
            logger.error(f"Error processing batch: {e}")
            # Set exception for all requests in the batch
            for req in batch:
                if not req['future'].cancelled():
                    req['future'].set_exception(e)

    async def generate(self, prompt: str, **kwargs) -> str:
        """
        Add a generation request to the queue and wait for the result.
        
        Args:
            prompt: Input prompt for generation
            **kwargs: Additional generation parameters (currently unused)
            
        Returns:
            Generated text
            
        Raises:
            RuntimeError: If the batch processor is not running
            ValueError: If prompt is empty
        """
        if not self._running:
            raise RuntimeError("Batch processor is not running. Call start() first.")
            
        if not prompt or not prompt.strip():
            raise ValueError("Prompt cannot be empty")

        future = asyncio.Future()
        await self.queue.put({
            'prompt': prompt.strip(), 
            'future': future,
            **kwargs
        })
        
        try:
            return await future
        except Exception as e:
            logger.error(f"Generation failed for prompt: {prompt[:50]}... Error: {e}")
            raise

    def get_stats(self) -> Dict[str, Any]:
        """Get current statistics about the inference server."""
        return {
            "queue_size": self.queue.qsize(),
            "max_batch_size": self.max_batch_size,
            "timeout_ms": self.timeout * 1000,
            "running": self._running,
            "model_device": str(self.model.device) if hasattr(self.model, 'device') else 'unknown'
        }-e \n# === src/utils/monitoring.py ===\n
# src/utils/monitoring.py

import time
import logging
import threading
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from collections import deque
from datetime import datetime, timedelta
from enum import Enum

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    logging.warning("NumPy not available, using basic statistics")

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """Enumeration for model types."""
    LOCAL = "local"
    EXTERNAL = "external"
    UNKNOWN = "unknown"

@dataclass
class ExecutionMetrics:
    """Data class for a single execution metric."""
    timestamp: float
    latency_ms: float
    cost_cents: float
    cache_hit: bool
    model_used: str
    model_type: ModelType
    
    def __post_init__(self):
        """Validate metrics after initialization."""
        if self.latency_ms < 0:
            raise ValueError("Latency cannot be negative")
        if self.cost_cents < 0:
            raise ValueError("Cost cannot be negative")

class MetricsCollector:
    """Thread-safe collector for performance and quality metrics with sliding window support."""
    
    def __init__(
        self, 
        max_history: int = 10000,
        window_hours: float = 24.0,
        enable_detailed_tracking: bool = True
    ):
        """
        Initialize metrics collector.
        
        Args:
            max_history: Maximum number of metrics to keep in memory
            window_hours: Time window for metrics (older metrics are discarded)
            enable_detailed_tracking: Whether to track detailed per-request metrics
        """
        self._lock = threading.RLock()
        self.max_history = max_history
        self.window_hours = window_hours
        self.enable_detailed_tracking = enable_detailed_tracking
        
        # Use deque for efficient FIFO operations
        self._metrics_history: deque = deque(maxlen=max_history)
        
        # Aggregate counters (thread-safe)
        self._cache_hits = 0
        self._total_requests = 0
        self._local_count = 0
        self._external_count = 0
        self._total_cost_cents = 0.0
        self._total_latency_ms = 0.0
        
        # Router accuracy tracking
        self._router_correct_decisions = 0
        self._router_total_decisions = 0
        
        # Start time for rate calculations
        self._start_time = time.time()
        
        logger.info(f"MetricsCollector initialized with max_history={max_history}, window_hours={window_hours}")

    def track_execution(
        self, 
        latency_s: float, 
        cost_cents: float, 
        cache_hit: bool, 
        model_used: str,
        additional_data: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        Track metrics for a single execution.
        
        Args:
            latency_s: Execution latency in seconds
            cost_cents: Cost in cents
            cache_hit: Whether this was a cache hit
            model_used: Name/identifier of the model used
            additional_data: Optional additional metadata
        """
        try:
            # Validate inputs
            if latency_s < 0:
                logger.warning(f"Negative latency received: {latency_s}s, setting to 0")
                latency_s = 0
            
            if cost_cents < 0:
                logger.warning(f"Negative cost received: {cost_cents} cents, setting to 0")
                cost_cents = 0
                
            latency_ms = latency_s * 1000
            timestamp = time.time()
            model_type = self._determine_model_type(model_used)
            
            with self._lock:
                # Update aggregate counters
                self._total_requests += 1
                self._total_latency_ms += latency_ms
                self._total_cost_cents += cost_cents
                
                if cache_hit:
                    self._cache_hits += 1
                
                if model_type == ModelType.LOCAL:
                    self._local_count += 1
                elif model_type == ModelType.EXTERNAL:
                    self._external_count += 1
                
                # Store detailed metrics if enabled
                if self.enable_detailed_tracking:
                    try:
                        metric = ExecutionMetrics(
                            timestamp=timestamp,
                            latency_ms=latency_ms,
                            cost_cents=cost_cents,
                            cache_hit=cache_hit,
                            model_used=model_used,
                            model_type=model_type
                        )
                        self._metrics_history.append(metric)
                    except ValueError as e:
                        logger.error(f"Invalid metrics data: {e}")
                        return
                
                # Clean up old data
                self._cleanup_old_metrics()
                
        except Exception as e:
            logger.error(f"Error tracking execution metrics: {e}")

    def track_router_decision(self, correct: bool) -> None:
        """
        Track router accuracy.
        
        Args:
            correct: Whether the router decision was correct
        """
        with self._lock:
            self._router_total_decisions += 1
            if correct:
                self._router_correct_decisions += 1

    def _determine_model_type(self, model_used: str) -> ModelType:
        """Determine if a model is local or external based on its name."""
        model_lower = model_used.lower()
        if any(keyword in model_lower for keyword in ['local', 'huggingface', 'hf', 'ollama', 'llamacpp']):
            return ModelType.LOCAL
        elif any(keyword in model_lower for keyword in ['openai', 'anthropic', 'claude', 'gpt', 'gemini', 'grok']):
            return ModelType.EXTERNAL
        else:
            return ModelType.UNKNOWN

    def _cleanup_old_metrics(self) -> None:
        """Remove metrics older than the window."""
        if not self.enable_detailed_tracking or not self._metrics_history:
            return
            
        cutoff_time = time.time() - (self.window_hours * 3600)
        
        # Remove old metrics from the front of deque
        while self._metrics_history and self._metrics_history[0].timestamp < cutoff_time:
            self._metrics_history.popleft()

    def _calculate_percentiles(self, values: List[float], percentiles: List[int]) -> Dict[str, float]:
        """Calculate percentiles, handling the case where numpy might not be available."""
        if not values:
            return {f"p{p}": 0.0 for p in percentiles}
        
        if HAS_NUMPY:
            return {f"p{p}": float(np.percentile(values, p)) for p in percentiles}
        else:
            # Fallback implementation without numpy
            sorted_values = sorted(values)
            n = len(sorted_values)
            result = {}
            
            for p in percentiles:
                if p == 0:
                    result[f"p{p}"] = sorted_values[0]
                elif p == 100:
                    result[f"p{p}"] = sorted_values[-1]
                else:
                    index = int((p / 100.0) * (n - 1))
                    result[f"p{p}"] = sorted_values[index]
            
            return result

    def get_report(self, include_detailed: bool = True) -> Dict[str, Any]:
        """
        Generate a comprehensive metrics report.
        
        Args:
            include_detailed: Whether to include detailed percentile calculations
            
        Returns:
            Dictionary containing metrics report
        """
        with self._lock:
            if self._total_requests == 0:
                return {
                    "message": "No data collected yet.",
                    "timestamp": datetime.now().isoformat()
                }
            
            current_time = time.time()
            uptime_hours = (current_time - self._start_time) / 3600
            
            # Basic metrics
            report = {
                "timestamp": datetime.now().isoformat(),
                "uptime_hours": round(uptime_hours, 2),
                "total_requests": self._total_requests,
                "requests_per_hour": round(self._total_requests / max(uptime_hours, 0.001), 2),
                
                # Cost metrics
                "avg_cost_per_query_cents": round(self._total_cost_cents / self._total_requests, 4),
                "total_cost_dollars": round(self._total_cost_cents / 100, 2),
                
                # Cache metrics
                "cache_hit_rate": round(self._cache_hits / self._total_requests, 4),
                "cache_hits": self._cache_hits,
                
                # Model distribution
                "local_requests": self._local_count,
                "external_requests": self._external_count,
                "local_vs_external_ratio": f"{self._local_count}:{self._external_count}",
                "local_percentage": round((self._local_count / self._total_requests) * 100, 1),
                
                # Average latency
                "avg_latency_ms": round(self._total_latency_ms / self._total_requests, 2)
            }
            
            # Router accuracy
            if self._router_total_decisions > 0:
                report["router_accuracy"] = round(
                    self._router_correct_decisions / self._router_total_decisions, 4
                )
                report["router_decisions"] = self._router_total_decisions
            
            # Detailed metrics (if enabled and requested)
            if include_detailed and self.enable_detailed_tracking and self._metrics_history:
                recent_metrics = [m for m in self._metrics_history 
                                if current_time - m.timestamp <= self.window_hours * 3600]
                
                if recent_metrics:
                    latencies = [m.latency_ms for m in recent_metrics]
                    costs = [m.cost_cents for m in recent_metrics]
                    
                    # Latency percentiles
                    latency_percentiles = self._calculate_percentiles(latencies, [50, 95, 99])
                    report.update({
                        f"latency_{k}_ms": round(v, 2) for k, v in latency_percentiles.items()
                    })
                    
                    # Cost percentiles
                    cost_percentiles = self._calculate_percentiles(costs, [50, 95, 99])
                    report.update({
                        f"cost_{k}_cents": round(v, 4) for k, v in cost_percentiles.items()
                    })
                    
                    # Recent window stats
                    report["recent_window_hours"] = self.window_hours
                    report["recent_requests"] = len(recent_metrics)
            
            return report

    def reset_metrics(self) -> None:
        """Reset all collected metrics."""
        with self._lock:
            self._metrics_history.clear()
            self._cache_hits = 0
            self._total_requests = 0
            self._local_count = 0
            self._external_count = 0
            self._total_cost_cents = 0.0
            self._total_latency_ms = 0.0
            self._router_correct_decisions = 0
            self._router_total_decisions = 0
            self._start_time = time.time()
            
        logger.info("Metrics reset successfully")

    def export_raw_data(self) -> List[Dict[str, Any]]:
        """
        Export raw metrics data for external analysis.
        
        Returns:
            List of dictionaries containing raw metric data
        """
        with self._lock:
            if not self.enable_detailed_tracking:
                logger.warning("Detailed tracking is disabled, no raw data available")
                return []
            
            return [
                {
                    "timestamp": m.timestamp,
                    "datetime": datetime.fromtimestamp(m.timestamp).isoformat(),
                    "latency_ms": m.latency_ms,
                    "cost_cents": m.cost_cents,
                    "cache_hit": m.cache_hit,
                    "model_used": m.model_used,
                    "model_type": m.model_type.value
                }
                for m in self._metrics_history
            ]

    def get_health_status(self) -> Dict[str, Any]:
        """
        Get current health status of the metrics collector.
        
        Returns:
            Dictionary containing health information
        """
        with self._lock:
            current_time = time.time()
            
            # Check if we're receiving recent data
            last_request_time = None
            if self._metrics_history:
                last_request_time = self._metrics_history[-1].timestamp
                time_since_last = current_time - last_request_time
            else:
                time_since_last = current_time - self._start_time
            
            return {
                "status": "healthy" if time_since_last < 300 else "idle",  # 5 minutes threshold
                "total_requests": self._total_requests,
                "memory_usage_metrics": len(self._metrics_history),
                "max_memory_usage": self.max_history,
                "memory_utilization": round(len(self._metrics_history) / self.max_history, 2),
                "time_since_last_request_seconds": round(time_since_last, 1),
                "last_request_time": datetime.fromtimestamp(last_request_time).isoformat() if last_request_time else None
            }